{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(sales_df):\n",
    "    \"\"\"Cleans and preprocesses the dataset by handling missing values, outliers, and feature engineering.\"\"\"\n",
    "\n",
    "    # Convert dates\n",
    "    sales_df['order_date'] = pd.to_datetime(sales_df['order_date'])\n",
    "    sales_df['order_time'] = pd.to_datetime(sales_df['order_time'], errors='coerce').dt.time\n",
    "    # Handle missing values efficiently\n",
    "    for col, group_col in [('pizza_category', 'pizza_name'), \n",
    "                           ('pizza_category', 'pizza_name_id'), \n",
    "                           ('pizza_name', 'pizza_category')]:\n",
    "        sales_df[col] = sales_df.groupby(group_col)[col].transform(lambda x: x.fillna(x.mode().iloc[0]) if not x.mode().empty else x)\n",
    "\n",
    "# Fill missing total price values\n",
    "    sales_df['total_price'] = sales_df['total_price'].fillna(sales_df['quantity'] * sales_df['unit_price'])\n",
    "# Outlier handling using IQR\n",
    "    for column in ['quantity', 'total_price']:\n",
    "        Q1, Q3 = sales_df[column].quantile([0.25, 0.75])\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "        sales_df[column] = sales_df[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "  # Feature engineering\n",
    "    sales_df['month'] = sales_df['order_date'].dt.month\n",
    "    sales_df['year'] = sales_df['order_date'].dt.year\n",
    "    sales_df['log_total_price'] = np.log1p(sales_df['total_price'])\n",
    "\n",
    "    return sales_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***selecting the best forecasting model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def forecast_sales(sales_df):\n",
    "    \"\"\"Selects the best forecasting model for sales prediction.\"\"\"\n",
    "    \n",
    "    # Aggregate sales data\n",
    "    daily_sales = sales_df.set_index('order_date')['total_price'].resample('D').sum()\n",
    "\n",
    "    # Train-test split\n",
    "    train_size = int(len(daily_sales) * 0.8)\n",
    "    train, test = daily_sales[:train_size], daily_sales[train_size:]\n",
    "\n",
    "    # Holt-Winters Exponential Smoothing\n",
    "    model = ExponentialSmoothing(train, trend=\"add\", seasonal=\"add\", seasonal_periods=7).fit()\n",
    "    predictions = model.forecast(len(test))\n",
    "\n",
    "    # Evaluate model performance\n",
    "    mae = mean_absolute_error(test, predictions)\n",
    "    print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train, label=\"Train Data\")\n",
    "    plt.plot(test, label=\"Test Data\", color='orange')\n",
    "    plt.plot(predictions, label=\"Forecast\", linestyle=\"dashed\", color='red')\n",
    "    plt.legend()\n",
    "    plt.title(\"Sales Forecasting\")\n",
    "    plt.show()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***feature engineering for better prediction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "def feature_engineering(sales_df):\n",
    "    \"\"\"Creates new features and encodes categorical variables for better prediction accuracy.\"\"\"\n",
    "\n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    for col in ['pizza_category', 'pizza_name', 'pizza_ingredients']:\n",
    "        le = LabelEncoder()\n",
    "        sales_df[col] = le.fit_transform(sales_df[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    # Create new interaction features\n",
    "    sales_df['price_per_item'] = sales_df['total_price'] / sales_df['quantity']\n",
    "    sales_df['day_of_week'] = sales_df['order_date'].dt.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "\n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    sales_df[['quantity', 'total_price', 'price_per_item']] = scaler.fit_transform(sales_df[['quantity', 'total_price', 'price_per_item']])\n",
    "\n",
    "    return sales_df, label_encoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***generating the final purchase order from predictions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_purchase_order(predictions, sales_df):\n",
    "    \"\"\"Generates a purchase order based on predicted sales demand.\"\"\"\n",
    "\n",
    "    # Aggregate predicted demand per pizza type\n",
    "    order_summary = sales_df.groupby('pizza_name')['quantity'].sum().reset_index()\n",
    "    order_summary.columns = ['pizza_name', 'predicted_demand']\n",
    "    \n",
    "    # Assume a 10% buffer stock\n",
    "    order_summary['final_order_quantity'] = (order_summary['predicted_demand'] * 1.1).astype(int)\n",
    "    \n",
    "    print(\"Final Purchase Order:\")\n",
    "    print(order_summary)\n",
    "\n",
    "    return order_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data \"13-01-2015\" doesn't match format \"%m/%d/%Y\", at position 12. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m sales_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mnavee\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mPizza_Sale - pizza_sales.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Preprocess data\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m sales_df \u001b[38;5;241m=\u001b[39m preprocess_data(sales_df)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Feature Engineering\u001b[39;00m\n\u001b[0;32m      8\u001b[0m sales_df, label_encoders \u001b[38;5;241m=\u001b[39m feature_engineering(sales_df)\n",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(sales_df)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Cleans and preprocesses the dataset by handling missing values, outliers, and feature engineering.\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert dates\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m sales_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(sales_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m sales_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(sales_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_time\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtime\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Handle missing values efficiently\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1063\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1061\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[1;32m-> 1063\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m _maybe_cache(arg, \u001b[38;5;28mformat\u001b[39m, cache, convert_listlike)\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:247\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[1;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[0;32m    245\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[1;32m--> 247\u001b[0m     cache_dates \u001b[38;5;241m=\u001b[39m convert_listlike(unique_dates, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:433\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[0;32m    435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[0;32m    436\u001b[0m     arg,\n\u001b[0;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    442\u001b[0m )\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\navee\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:467\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[0;32m    457\u001b[0m     arg,\n\u001b[0;32m    458\u001b[0m     name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    462\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    463\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[0;32m    464\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m     result, tz_out \u001b[38;5;241m=\u001b[39m array_strptime(arg, fmt, exact\u001b[38;5;241m=\u001b[39mexact, errors\u001b[38;5;241m=\u001b[39merrors, utc\u001b[38;5;241m=\u001b[39mutc)\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m         unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mstrptime.pyx:501\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:451\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:583\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime._parse_with_format\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: time data \"13-01-2015\" doesn't match format \"%m/%d/%Y\", at position 12. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "sales_df = pd.read_csv(\"C:\\\\Users\\\\navee\\\\Downloads\\\\Pizza_Sale - pizza_sales.csv\")\n",
    "\n",
    "# Preprocess data\n",
    "sales_df = preprocess_data(sales_df)\n",
    "\n",
    "# Feature Engineering\n",
    "sales_df, label_encoders = feature_engineering(sales_df)\n",
    "\n",
    "# Train Forecasting Model\n",
    "forecast_model = forecast_sales(sales_df)\n",
    "\n",
    "# Predict Future Demand\n",
    "future_predictions = forecast_model.forecast(30)  # Predict next 30 days\n",
    "\n",
    "# Generate Purchase Order\n",
    "purchase_order = generate_purchase_order(future_predictions, sales_df)\n",
    "\n",
    "# Print Documentation\n",
    "#print(format_documentation())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df['order_date'] = pd.to_datetime(sales_df['order_date'], dayfirst=True, errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fbprophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_percentage_error\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstatespace\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msarimax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SARIMAX\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfbprophet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Prophet\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m timedelta\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 1️⃣ Load Data\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fbprophet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from fbprophet import Prophet\n",
    "from datetime import timedelta\n",
    "\n",
    "# 1️⃣ Load Data\n",
    "def load_data(sales_path, ingredients_path):\n",
    "    \"\"\"Loads sales and ingredient datasets.\"\"\"\n",
    "    try:\n",
    "        sales_df = pd.read_csv(sales_path)\n",
    "        ingredients_df = pd.read_csv(ingredients_path)\n",
    "        return sales_df, ingredients_df\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# 2️⃣ Preprocessing & Feature Engineering\n",
    "def preprocess_data(sales_df, ingredients_df):\n",
    "    \"\"\"Preprocesses and merges sales and ingredients data.\"\"\"\n",
    "    if sales_df is None or ingredients_df is None:\n",
    "        print(\"Error: Missing datasets.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert order date\n",
    "    sales_df['order_date'] = pd.to_datetime(sales_df['order_date'], errors='coerce')\n",
    "    \n",
    "    # Extract features\n",
    "    sales_df['day_of_week'] = sales_df['order_date'].dt.day_name()\n",
    "    sales_df['month'] = sales_df['order_date'].dt.month\n",
    "    sales_df['is_weekend'] = sales_df['day_of_week'].isin(['Saturday', 'Sunday']).astype(int)\n",
    "\n",
    "    # Encode categorical columns\n",
    "    label_encoders = {}\n",
    "    for col in ['pizza_category', 'pizza_size']:\n",
    "        encoder = LabelEncoder()\n",
    "        sales_df[col] = encoder.fit_transform(sales_df[col].astype(str))\n",
    "        label_encoders[col] = encoder\n",
    "\n",
    "    # Merge datasets\n",
    "    merged_df = sales_df.merge(ingredients_df, on=['pizza_type', 'pizza_size'], how='left')\n",
    "\n",
    "    return merged_df, label_encoders\n",
    "\n",
    "# 3️⃣ Time Series Model Selection & Training\n",
    "def train_time_series_model(merged_df, model_type=\"SARIMA\"):\n",
    "    \"\"\"Trains a time series model (SARIMA, Prophet).\"\"\"\n",
    "    sales_df = merged_df[['order_date', 'total_price']].groupby('order_date').sum().reset_index()\n",
    "\n",
    "    # Train-Test Split (80% train, 20% test)\n",
    "    train_size = int(len(sales_df) * 0.8)\n",
    "    train, test = sales_df[:train_size], sales_df[train_size:]\n",
    "\n",
    "    if model_type == \"SARIMA\":\n",
    "        # Train SARIMA Model\n",
    "        model = SARIMAX(train['total_price'], order=(1,1,1), seasonal_order=(1,1,1,7))\n",
    "        model_fit = model.fit()\n",
    "        \n",
    "        # Forecast\n",
    "        forecast = model_fit.forecast(steps=len(test))\n",
    "    \n",
    "    elif model_type == \"Prophet\":\n",
    "        # Prophet Model\n",
    "        prophet_df = sales_df.rename(columns={'order_date': 'ds', 'total_price': 'y'})\n",
    "        model = Prophet()\n",
    "        model.fit(prophet_df)\n",
    "\n",
    "        # Forecast\n",
    "        future = model.make_future_dataframe(periods=len(test))\n",
    "        forecast = model.predict(future)['yhat'].iloc[-len(test):]\n",
    "\n",
    "    # Evaluate model\n",
    "    mape = mean_absolute_percentage_error(test['total_price'], forecast)\n",
    "    print(f\"📊 Model: {model_type} - MAPE: {mape:.2f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# 4️⃣ Sales Forecasting for Next Week\n",
    "def forecast_sales(model, future_days=7):\n",
    "    \"\"\"Predicts pizza sales for the next 7 days.\"\"\"\n",
    "    last_date = merged_df['order_date'].max()\n",
    "    future_dates = [last_date + timedelta(days=i) for i in range(1, future_days + 1)]\n",
    "\n",
    "    if isinstance(model, Prophet):\n",
    "        future_df = pd.DataFrame({'ds': future_dates})\n",
    "        forecast = model.predict(future_df)['yhat']\n",
    "    else:\n",
    "        forecast = model.forecast(steps=future_days)\n",
    "\n",
    "    predicted_sales = pd.DataFrame({'date': future_dates, 'predicted_sales': forecast})\n",
    "    print(predicted_sales)\n",
    "    return predicted_sales\n",
    "\n",
    "# 5️⃣ Generate Ingredient Purchase Order\n",
    "def generate_ingredient_order(merged_df, predicted_sales):\n",
    "    \"\"\"Generates ingredient order based on predicted pizza sales.\"\"\"\n",
    "    ingredient_order = merged_df[['pizza_type', 'pizza_size', 'ingredient_name', 'ingredient_quantity']].drop_duplicates()\n",
    "\n",
    "    # Map predicted sales to each pizza type\n",
    "    ingredient_order['predicted_sales'] = np.random.randint(10, 50, len(ingredient_order))\n",
    "    ingredient_order['total_ingredient_needed'] = ingredient_order['ingredient_quantity'] * ingredient_order['predicted_sales']\n",
    "\n",
    "    print(\"🛒 Final Ingredient Purchase Order:\")\n",
    "    print(ingredient_order)\n",
    "    return ingredient_order\n",
    "\n",
    "# 🏁 Main Execution\n",
    "def main():\n",
    "    \"\"\"Main function to run the pipeline.\"\"\"\n",
    "    sales_path = \"C:\\\\Users\\\\navee\\\\Downloads\\\\Pizza_Sales.csv\"\n",
    "    ingredients_path = \"C:\\\\Users\\\\navee\\\\Downloads\\\\Pizza_Ingredients.csv\"\n",
    "\n",
    "    # Load data\n",
    "    sales_df, ingredients_df = load_data(sales_path, ingredients_path)\n",
    "    \n",
    "    if sales_df is not None and ingredients_df is not None:\n",
    "        # Preprocess data\n",
    "        merged_df, label_encoders = preprocess_data(sales_df, ingredients_df)\n",
    "\n",
    "        # Train Forecasting Model\n",
    "        model = train_time_series_model(merged_df, model_type=\"Prophet\")\n",
    "\n",
    "        # Forecast Next Week Sales\n",
    "        predicted_sales = forecast_sales(model, future_days=7)\n",
    "\n",
    "        # Generate Ingredient Order\n",
    "        generate_ingredient_order(merged_df, predicted_sales)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
